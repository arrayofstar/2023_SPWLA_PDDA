{"data_path": "./data", "checkpoints": "./checkpoints/", "seq_len": 224, "label_len": 224, "shift_len": 0, "train_columns": ["DEPT", "GR", "RHOB", "NPHI", "log_RD"], "label_columns": ["RHOB_pred", "NPHI_pred", "log_RD_pred", "RHOB_dept_pred", "NPHI_dept_pred", "log_RD_dept_pred"], "data_augment_num": 3, "scale": true, "scaler": "Normalization", "train_data_mean": {"DEPT": 2802.9342726467985, "GR": 106.29523803666271, "RHOB": 2.485495370528771, "NPHI": 0.2681623599900425, "log_RD": 0.9080766672096289, "RHOB_pred": 2.485568917207632, "NPHI_pred": 0.26836596464050355, "log_RD_pred": 0.9077362966982049, "RHOB_dept_pred": 2804.6500504081832, "NPHI_dept_pred": 2805.83044715747, "log_RD_dept_pred": 2804.6606023163176}, "train_data_std": {"DEPT": 1297.8616061713035, "GR": 30.03512080510817, "RHOB": 0.13115476363690626, "NPHI": 0.07725974131537536, "log_RD": 0.3837224532860237, "RHOB_pred": 0.13130638702661393, "NPHI_pred": 0.07727404290831486, "log_RD_pred": 0.3856919343381126, "RHOB_dept_pred": 1298.5704523151096, "NPHI_dept_pred": 1298.325104605418, "log_RD_dept_pred": 1297.853607505924}, "train_data_max": {"DEPT": 5743.0, "GR": 400.0, "RHOB": 3.0454828878476827, "NPHI": 0.691071165194212, "log_RD": 4.697986250135933, "RHOB_pred": 3.0454, "NPHI_pred": 0.6911, "log_RD_pred": 4.697674327602022, "RHOB_dept_pred": 5743.0, "NPHI_dept_pred": 5743.0, "log_RD_dept_pred": 5743.0}, "train_data_min": {"DEPT": 395.0, "GR": 7.345, "RHOB": 1.2664603293363685, "NPHI": -0.01000062686604607, "log_RD": -0.6761617176940682, "RHOB_pred": 1.2666, "NPHI_pred": -0.0099, "log_RD_pred": -0.6761293934594911, "RHOB_dept_pred": 395.0, "NPHI_dept_pred": 395.0, "log_RD_dept_pred": 395.0}, "use_deep_learning": true, "dtw_after_predict": true, "model": "seq_blstm", "model_para_num": 600067, "mWDN_level": 3, "mWDN_rnn_hidden_size": 256, "mWDN_rnn_num_layers": 1, "mWDN_fcn_hidden_size": 256, "mWDN_alpha": 0.5, "mWDN_beta": 0.5, "dtw_method": "dtwalign", "dtw_window_size": 180, "dtw_window_type": "sakoechiba", "dtw_step_pattern": "symmetricP05", "train_epochs": 100, "batch_size": 128, "optimizer": "adam", "lr": 0.001, "patience": 5, "resume": "seq_blstm-0.00591.pth", "output_path": "./results/", "log_file": "./results/train_log\\seq_blstm-seq(224)-label(224).log", "seed": 13, "use_gpu": true, "train_val_time": 0.0}